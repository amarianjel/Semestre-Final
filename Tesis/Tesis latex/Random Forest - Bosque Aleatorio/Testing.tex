    \hypertarget{random-forest---testeo-del-algoritmo}{%
\section{Random Forest - Testeo del algoritmo}\label{random-forest---testeo-del-algoritmo}}

	En esta sección se ralizará el Testing del algoritmo Random Forest como en la sección \ref{nauxefve-bayes---testeo-del-algoritmo}.\\

    \hypertarget{predicciones-sobre-los-datos-del-testing-y-muxe9tricas-de-rendimiento}{%
\subsection{Predicciones sobre los datos del testing y métricas de rendimiento}\label{predicciones-sobre-los-datos-del-testing-y-muxe9tricas-de-rendimiento}}

	Tal como, la sección \ref{NBT:predicciones-sobre-los-datos-del-testing-y-muxe9tricas-de-rendimiento} se realizará la evaluación de métricas del Testing.\\

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{60}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tests \PYZhy{} Presición :}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{prediccionTests}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tests \PYZhy{} Reporte de clasificación:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{.}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{prediccionTests}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Tests - Presición : 0.6666666666666666
Tests - Reporte de clasificación:
               precision    recall  f1-score   support

           0       0.60      1.00      0.75         6
           1       1.00      0.33      0.50         6

    accuracy                           0.67        12
   macro avg       0.80      0.67      0.62        12
weighted avg       0.80      0.67      0.62        12

    \end{Verbatim}

	Por cada estado (0 y 1) la precisión de los datos del testing en el modelo tiene un valor de 60\%  y 100\% para cada estado respectivo en predicción. La exhaustividad informa la cantidad de datos capaz de identificar y, en este caso, es de un 100\% y 33\% para cada estado respectivo y, finalmente, el F1 combina los valores de precisión y exhaustividad obteniéndose un 75\% y 50\% en los estados respectivos. 
\par Lo que se busca es la precisión del modelo, por consecuencia, el algoritmo de ML Logistic Regression tiene una precisión del 66,66\% de predicción en los 12 pacientes de muestra del testing.\\

    \hypertarget{matriz-de-confusiuxf3n}{%
\subsection{Matriz de Confusión}\label{matriz-de-confusiuxf3n}}

Evaluaremos la matriz de confusión que se elaboró con los datos del testing.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{matriz} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{prediccionTests}\PY{p}{)}

\PY{n}{plot\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{conf\PYZus{}mat}\PY{o}{=}\PY{n}{matriz}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,} \PY{n}{show\PYZus{}normed}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

\begin{center}
    	\begin{figure}[H]
	\centering
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{Random Forest - Bosque Aleatorio/output_100_0.png}
	\caption{Matriz de confusión de testing Random Forest}
	\label{fig:mctrf}
	\end{figure}
\end{center}
    
   	En la matriz de confusión \ref{fig:mctrf}, los valores de la diagonal principal (0,0) = 6 y (1,1) = 2 corresponden con los valores estimados de forma correcta por el modelo, tanto los TN, como los TP. La otra diagonal, representa los casos en los que el modelo \textit{"se ha equivocado"}, según la matriz de confusión \ref{fig:mcenb} son (0,1) = 0 FP y (1,0) = 4 FN.
\par Las afirmaciones anteriores sugieren que la las predicciones son altas, pero también existen algunos errores en la predicción.
\par Respecto al ACV, el modelo identificó a 6 pacientes que posee un buen pronóstico (estable) y 2 paciente que posee un pronóstico no tan favorable, según la variable objetivo detallada en \ref{crear-columna-para-nihss_alta_estable_o_grave}. Así mismo, el modelo identifica a 4 pacientes que poseen buen pronóstico, pero en realidad poseen mal pronóstico.\\

    \hypertarget{random-forest---uso-del-algoritmo}{%
\section{Random Forest - Uso del algoritmo}\label{random-forest---uso-del-algoritmo}}
	
	Al igual que en la sección \ref{nauxefve-bayes---uso-del-algoritmo}, realizaremos comparaciones de predicción y probabilidad. Además se incorporará métricas especiales para árboles de su libreria en esta sección.

    \hypertarget{importancia-de-los-predictores}{%
\subsection{Importancia de los predictores}\label{importancia-de-los-predictores}}

	 Al igual que la sub sección \ref{NBT:importancia-de-los-predictores}, realizaremos los procedimientos.\\

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Predicciones probabilísticas}
\PY{c+c1}{\PYZsh{} ===============================}
\PY{c+c1}{\PYZsh{} Con .predict\PYZus{}proba() se obtiene, para cada observación, la probabilidad predicha}
\PY{c+c1}{\PYZsh{} de pertenecer a cada una de las dos clases.}
\PY{n}{predicciones} \PY{o}{=} \PY{n}{rfc}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{predicciones} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{predicciones}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{n}{rfc}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{)}
\PY{n}{predicciones}
\end{Verbatim}
\end{tcolorbox}

\begin{table}[H]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{0.2\textwidth}{!}{
\begin{tabular}{|c|l|l|}
\hline
\multicolumn{1}{|l|}{} & \multicolumn{1}{c|}{\textbf{0}} & \multicolumn{1}{c|}{\textbf{1}} \\ \hline
\textbf{0} & 0,53 & 0,47 \\ \hline
\textbf{1} & 0,81 & 0,19 \\ \hline
\textbf{2} & 0,86 & 0,14 \\ \hline
\textbf{3} & 0,85 & 0,15 \\ \hline
\textbf{4} & 0,60 & 0,40 \\ \hline
\textbf{5} & 0,52 & 0,48 \\ \hline
\textbf{6} & 0,29 & 0,71 \\ \hline
\textbf{7} & 0,62 & 0,38 \\ \hline
\textbf{8} & 0,51 & 0,49 \\ \hline
\textbf{9} & 0,28 & 0,72 \\ \hline
\textbf{10} & 0,97 & 0,03 \\ \hline
\textbf{11} & 0,95 & 0,05 \\ \hline
\end{tabular}%
}
\caption{Predicciones probabilísticas para cada observación Random Forest}
\label{tab:clasificacin rf}
\end{table}
        
    De acuerdo a lo mostrado en la tabla \ref{tab:probabilistica rf}, se observa como ejemplo, que la tupla 0 posee un 53\% de probabilidad para el estado 0 y un 47\% de probabilidad para el estado 1, en otras palabras, el paciente posee un buen pronóstico con un 53\% de probabilidad para este algoritmo.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Predicciones con clasificación final}
\PY{c+c1}{\PYZsh{} ==============================================================================}
\PY{c+c1}{\PYZsh{} Con .predict() se obtiene, para cada observación, la clasificación predicha por}
\PY{c+c1}{\PYZsh{} el modelo. Esta clasificación se corresponde con la clase con mayor probabilidad.}
\PY{n}{predicciones} \PY{o}{=} \PY{n}{rfc}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{predicciones} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{predicciones}\PY{p}{)}
\PY{n}{predicciones}
\end{Verbatim}
\end{tcolorbox}

\begin{table}[H]
\centering
\setlength{\tabcolsep}{10pt}
\resizebox{0.12\textwidth}{!}{
\begin{tabular}{|c|l|}
\hline
\multicolumn{1}{|l|}{} & \multicolumn{1}{c|}{\textbf{0}} \\ \hline
\textbf{0} & 0 \\ \hline
\textbf{1} & 0 \\ \hline
\textbf{2} & 0 \\ \hline
\textbf{3} & 0 \\ \hline
\textbf{4} & 0 \\ \hline
\textbf{5} & 0 \\ \hline
\textbf{6} & 1 \\ \hline
\textbf{7} & 0 \\ \hline
\textbf{8} & 0 \\ \hline
\textbf{9} & 1 \\ \hline
\textbf{10} & 0 \\ \hline
\textbf{11} & 0 \\ \hline
\textbf{12} & 0 \\ \hline
\end{tabular}%
}
\caption{Predicciones probabilísticas con clasificación final Random Forest}
\label{tab:probabilistica rf}
\end{table}
        
	En la tabla \ref{tab:clasificacion dt}, se observa para interpretación como ejemplo la tupla número 6. Esta tupla no pertence al estado 0, por ende, en términos de ACV, el paciente número 6 no tiene un pronóstico favorable. 
\par Recordamos que la Matriz de Confusión \ref{fig:mctrf} poseía 2 pacientes que tenian un pronóstico menos favorable, ellos son los que se registran con el valor 1 en la columna 0 de la tabla.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{importancia\PYZus{}predictores} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}
                            \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predictor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{dataset}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{NIHSS\PYZus{}alta\PYZus{}ESTABLE\PYZus{}O\PYZus{}GRAVE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{columns}\PY{p}{,}
                             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{importancia}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{rfc}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}}\PY{p}{\PYZcb{}}
                            \PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Importancia de los predictores en el modelo}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{importancia\PYZus{}predictores}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{importancia}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

\begin{table}[H]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{|c|l|l|}
\hline
\multicolumn{1}{|l|}{} & \multicolumn{1}{c|}{\textbf{Predictor}} & \multicolumn{1}{c|}{\textbf{importancia}} \\ \hline
\textbf{11} & NIHSS\_alta\_cat & 0,159261 \\ \hline
\textbf{2} & EDAD & 0,103086 \\ \hline
\textbf{9} & NIHSS INICO ACV & 0,080756 \\ \hline
\textbf{5} & TRIGLICERIDOS & 0,078785 \\ \hline
\textbf{10} & NIHSS alta ACV & 0,077216 \\ \hline
\textbf{7} & CONTEO G.B. & 0,066683 \\ \hline
\textbf{3} & GLUCOSA & 0,056563 \\ \hline
\textbf{8} & GLASGOW AL INICO ACV & 0,056080 \\ \hline
\textbf{6} & INR & 0,055534 \\ \hline
\textbf{1} & DIABETES & 0,054373 \\ \hline
\textbf{4} & COL. TOTAL & 0,053273 \\ \hline
\textbf{17} & GLUCOSA\_cat & 0,051421 \\ \hline
\textbf{15} & TRIGLICERIDOS\_cat & 0,032218 \\ \hline
\textbf{13} & CONTEO G.B.\_cat & 0,023646 \\ \hline
\textbf{12} & GLASGOW\_cat & 0,010939 \\ \hline
\textbf{22} & NIHSS\_INICIO\_cat\_Leve (Trombolisando) & 0,008097 \\ \hline
\textbf{23} & NIHSS\_INICIO\_cat\_Moderado (Buen Pronostico) & 0,007339 \\ \hline
\textbf{21} & NIHSS\_INICIO\_cat\_Grave & 0,004856 \\ \hline
\textbf{16} & COL. TOTAL\_cat & 0,004865 \\ \hline
\textbf{0} & HTA & 0,004678 \\ \hline
\textbf{14} & INR\_cat & 0,003802 \\ \hline
\textbf{20} & NIHSS\_INICIO\_cat\_Déficit Mínimo & 0,002603 \\ \hline
\textbf{18} & EDAD\_cat & 0,002603 \\ \hline
\textbf{19} & NIHSS\_INICIO\_cat\_Déficit Importante & 0,001010 \\ \hline
\textbf{24} & NIHSS\_INICIO\_cat\_Sin Déficit & 0,000000 \\ \hline
\end{tabular}%
}
\caption{Importancia de los predictores Random Forest}
\label{tab:importancia predictor rf}
\end{table}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{65}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Get numerical feature importances}
\PY{n}{importances} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{rfc}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} List of tuples with variable and importance}
\PY{n}{feature\PYZus{}importances} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{feature}\PY{p}{,} \PY{n+nb}{round}\PY{p}{(}\PY{n}{importance}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{feature}\PY{p}{,} \PY{n}{importance} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{feature\PYZus{}list}\PY{p}{,} \PY{n}{importances}\PY{p}{)}\PY{p}{]}
\PY{c+c1}{\PYZsh{} Sort the feature importances by most important first}
\PY{n}{feature\PYZus{}importances} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{feature\PYZus{}importances}\PY{p}{,} \PY{n}{key} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{reverse} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Reset style }
\PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fivethirtyeight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} lista de x ubicaciones para trazar}
\PY{n}{x\PYZus{}values} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{importances}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Gráfico de barras}
\PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{,} \PY{n}{importances}\PY{p}{,} \PY{n}{orientation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vertical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{edgecolor} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth} \PY{o}{=} \PY{l+m+mf}{1.2}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Marque las etiquetas para el eje x}
\PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{,} \PY{n}{feature\PYZus{}list}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vertical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Etiquetas de eje y título}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importancia}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variable}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importancia de Variables}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    	\begin{figure}[H]
	\centering
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Random Forest - Bosque Aleatorio/output_108_0.png}
	\caption{Importancia de los predictores en Random Forest}
	\label{fig:iprf}
	\end{figure}
\end{center}

    Las variables que presentan gran importancia al momento de la predicción según la tabla \ref{tab:importancia predictor rf} y la Figura \ref{fig:iprf} son ``NIHSS\_alta\_cat'', ''EDAD'' y ``NIHSS alta ACV'', ``CONTEO G.B.\_cat'' con más del 34\% de importancia en la predicción. El mayor predictor es ``NIHSS\_alta\_cat'' con un 15,92\% de importancia para este modelo.\\
    

    \hypertarget{importancia-acumulada}{%
\subsection{Importancia acumulada}\label{importancia-acumulada}}

	Al igual que, en la sub sección \ref{DT:importancia-acumulada}, reduciremos al 95\% de los datos.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{66}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Lista de funciones ordenadas de mayor a menor importancia}
\PY{n}{sorted\PYZus{}importances} \PY{o}{=} \PY{p}{[}\PY{n}{importance}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{importance} \PY{o+ow}{in} \PY{n}{feature\PYZus{}importances}\PY{p}{]}
\PY{n}{sorted\PYZus{}features} \PY{o}{=} \PY{p}{[}\PY{n}{importance}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{for} \PY{n}{importance} \PY{o+ow}{in} \PY{n}{feature\PYZus{}importances}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Importancias acumulativas}
\PY{n}{cumulative\PYZus{}importances} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{cumsum}\PY{p}{(}\PY{n}{sorted\PYZus{}importances}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Haz un gráfico de líneas}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{,} \PY{n}{cumulative\PYZus{}importances}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Dibujar línea al 95\PYZpc{} de importancia retenida}
\PY{n}{plt}\PY{o}{.}\PY{n}{hlines}\PY{p}{(}\PY{n}{y} \PY{o}{=} \PY{l+m+mf}{0.95}\PY{p}{,} \PY{n}{xmin}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{xmax}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sorted\PYZus{}importances}\PY{p}{)}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyles} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dashed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Formato x ticks y etiquetas}
\PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{,} \PY{n}{sorted\PYZus{}features}\PY{p}{,} \PY{n}{rotation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vertical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Etiquetas de eje y título}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variable}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importancia acumulada}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importancia acumulada}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

\begin{center}
    	\begin{figure}[H]
	\centering
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Random Forest - Bosque Aleatorio/output_111_0.png}
	\caption{Importancia de los predictores acumulada en Random Forest}
	\label{fig:iparf}
	\end{figure}
\end{center}

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{67}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Encuentre el número de características para una importancia acumulada del 95\PYZpc{}}
\PY{c+c1}{\PYZsh{} Agregue 1 porque Python está indexado a cero}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Número de columna para el 95 }\PY{l+s+si}{\PYZpc{} d}\PY{l+s+s1}{e importancia:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{cumulative\PYZus{}importances} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.95}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Número de columna para el 95 \% de importancia: 14
    \end{Verbatim}

    En la Figura \ref{fig:iparf} de importancia acumulada, la curva se dispara con la ``CONTEO G.B.\_cat'' , ratificandosé con el resultado del 95\% de importancia medido anteriormente.\\

